@article{bertsimas2016,
author = "Bertsimas, Dimitris and King, Angela and Mazumder, Rahul",
doi = "10.1214/15-AOS1388",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "04",
number = "2",
pages = "813--852",
publisher = "The Institute of Mathematical Statistics",
title = "Best subset selection via a modern optimization lens",
url = "https://doi.org/10.1214/15-AOS1388",
volume = "44",
year = "2016"
}


@inproceedings{cao2018learnable,
title={Learnable Embedding Space for Efficient Neural Architecture Compression},
author={Shengcao Cao and Xiaofang Wang and Kris M. Kitani},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1xLN3C9YX},
}

@article{Rodriguez-Lujan:2010:QPF:1756006.1859900,
 author = {Rodriguez-Lujan, Irene and Huerta, Ramon and Elkan, Charles and Cruz, Carlos Santa},
 title = {Quadratic Programming Feature Selection},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2010},
 volume = {11},
 month = aug,
 year = {2010},
 issn = {1532-4435},
 pages = {1491--1516},
 numpages = {26},
 url = {http://dl.acm.org/citation.cfm?id=1756006.1859900},
 acmid = {1859900},
 publisher = {JMLR.org},
} 

@inproceedings{14e3662b1cd64554adaeb966bb2a44c4,
title = "Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution",
abstract = "Feature selection, as a preprocessing step to machine learning, is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection methods with respect to efficiency and effectiveness. In this work, we introduce a novel concept, predominant correlation, and propose a fast filter method which can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis. The efficiency and effectiveness of our method is demonstrated through extensive comparisons with other methods using real-world data of high dimensionality.",
author = "Lei Yu and Huan Liu",
year = "2003",
language = "English (US)",
isbn = "1577351894",
volume = "2",
pages = "856--863",
editor = "T. Fawcett and N. Mishra",
booktitle = "Proceedings, Twentieth International Conference on Machine Learning",
}

@inproceedings{inproceedings,
author = {Chen, Dong and Cao, Xudong and Wen, Fang and Sun, Jian},
year = {2013},
month = {06},
pages = {3025-3032},
title = {Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification},
journal = {Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.389}
}

@article{article,
author = {Manbari, Zhaleh and Akhlaghian Tab, Fardin and Salavati, Chiman},
year = {2019},
month = {06},
pages = {},
title = {Hybrid Fast Unsupervised Feature Selection for High-dimensional Data},
volume = {124},
journal = {Expert Systems with Applications},
doi = {10.1016/j.eswa.2019.01.016}
}

@Article{Gu2018,
author="Gu, Shenkai
and Cheng, Ran
and Jin, Yaochu",
title="Feature selection for high-dimensional classification using a competitive swarm optimizer",
journal="Soft Computing",
year="2018",
month="Feb",
day="01",
volume="22",
number="3",
pages="811--822",
abstract="When solving many machine learning problems such as classification, there exists a large number of input features. However, not all features are relevant for solving the problem, and sometimes, including irrelevant features may deteriorate the learning performance.Please check the edit made in the article title Therefore, it is essential to select the most relevant features, which is known as feature selection. Many feature selection algorithms have been developed, including evolutionary algorithms or particle swarm optimization (PSO) algorithms, to find a subset of the most important features for accomplishing a particular machine learning task. However, the traditional PSO does not perform well for large-scale optimization problems, which degrades the effectiveness of PSO for feature selection when the number of features dramatically increases. In this paper, we propose to use a very recent PSO variant, known as competitive swarm optimizer (CSO) that was dedicated to large-scale optimization, for solving high-dimensional feature selection problems. In addition, the CSO, which was originally developed for continuous optimization, is adapted to perform feature selection that can be considered as a combinatorial optimization problem. An archive technique is also introduced to reduce computational cost. Experiments on six benchmark datasets demonstrate that compared to the canonical PSO-based and a state-of-the-art PSO variant for feature selection, the proposed CSO-based feature selection algorithm not only selects a much smaller number of features, but result in better classification performance as well.",
issn="1433-7479",
doi="10.1007/s00500-016-2385-6",
url="https://doi.org/10.1007/s00500-016-2385-6"
}


@ARTICLE{5989810, 
author={Q. {Song} and J. {Ni} and G. {Wang}}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={A Fast Clustering-Based Feature Subset Selection Algorithm for High-Dimensional Data}, 
year={2013}, 
volume={25}, 
number={1}, 
pages={1-14}, 
keywords={data handling;graph theory;pattern clustering;fast clustering based feature subset selection algorithm;high dimensional data;feature selection;FAST;graph theoretic clustering methods;minimum spanning tree;MST;Clustering algorithms;Complexity theory;Markov processes;Prediction algorithms;Correlation;Feature extraction;Partitioning algorithms;Feature subset selection;filter method;feature clustering;graph-based clustering}, 
doi={10.1109/TKDE.2011.181}, 
ISSN={1041-4347}, 
month={Jan},}


@article{article1,
author = {Fan, Jianqing and Lv, Jinchi},
year = {2010},
month = {01},
pages = {101-148},
title = {A Selective Overview of Variable Selection in High Dimensional Feature Space},
volume = {20},
journal = {Statistica Sinica}
}

@article{article2,
author = {Pack Kaelbling, Leslie and Littman, Michael and P Moore, Andrew},
year = {1996},
month = {04},
pages = {237-285},
title = {Reinforcement Learning: A Survey},
volume = {4},
journal = {Journal of Artificial Intelligence Research}
}